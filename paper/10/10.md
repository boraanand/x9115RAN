#### Contributors:
- Nirmesh Khandelwal
- Anand Bora
- Ravi Singh

#### Abstract:
- As a part of this report, we analyze, how Search Based Software Engineering (SBSE) has contributed to debugging, maintenance, and testing of Web/ Mobile applications, and how it has evolved from 2008 to 2015.
- Identifying a software bug and then debugging the software is largely a manual and labor-intensive endeavor, though the process is tool-supported. One objective of SBSE is to automatically identify and fix bugs.
- Genetic programming has been used to search for repairs to programs by altering a few lines of source code. 
- Search-based software engineering has been applied to software testing, including automatic generation of test cases (test data), test case minimization and test case prioritization. Regression testing has also received some attention.

#### Key Words:
- **Search based Software Engineering**:
Is an approach to convert SE problems into optimization problems, which can then be solved by searching optimal solution(s) in the search space.
- **Automatic test generation**: 
Is the process of programmatically creating a set of data for testing the adequacy of new or revised software applications.
- **Hyper-heuristic search**:
Normal heuristics operate on search space of potential solutions. Hyper Heuristics operate on search space of heuristics.
- **Software testing**: 
Is an investigation conducted to provide stakeholders with information about the quality of the product or service under test. Test techniques include the process of executing a program or application with the intent of finding software bugs.

#### Introduction: 
In Search Based Software Engineering, the term ‘search’ is used to refer to the metaheuristic search-based optimisation techniques that are used. SBSE seeks to reformulate Software Engineering problems as search-based optimisation problems (or ‘search problems’ for short). This is not to be confused with textual or hypertextual searching. Rather, for Search Based Software Engineering, a search problem is one in which optimal or near optimal solutions are sought in a search space of candidate solutions, guided by a fitness function that distinguishes between better and worse solutions. 
Interest in search-based approaches in software engineering has been growing rapidly over the past years. Extensive work has been done especially in the field of software testing and related fields.  This survey will cover the branch of software design, where refactoring and modularization have also been taken into account as they are considered as actions of “re-designing” software. Software Engineering often considers problems that involve finding a suitable balance between competing and potentially conflicting goals. There is often a bewilderingly large set of choices and finding good solutions can be hard. For instance, the following is an illustrative list of Software Engineering questions. [3]

1. What is the smallest set of test cases that cover all branches in this program?
2. What is the best way to structure the architecture of this system?
3. What is the set of requirements that balances software development cost and customer satisfaction?
4. What is the best allocation of resources to this software development project?
5. What is the best sequence of refactoring steps to apply to this system?

Answers to these questions might be expected from literature on testing, design, requirements engineering, Software Engineering management and refactoring respectively. It would appear at first sight, that these questions involve different aspects of software engineering, would be covered by different conferences and specialized journals and would have little in common. However, all of these questions are essentially optimisation questions. As such, they are typical of the kinds of problem for which Search Based Software Engineering (SBSE) is well adapted and with which each has been successfully formulated as a search based optimisation problem. That is, as we shall see in this survey, SBSE has been applied to testing design, requirements, project management and refactoring. 

A good indicator of the greatest advantages over other sorbent-based techniques is the number of publications related with SBSE that strongly increased in the last years, and based on our reviews of papers in code 1 - 9, we try to explain the progression of SBSE over the years.

#### Background: R

#### Search Algorithms: 
To understand the basic concepts behind the approaches presented here, we briefly introduce genetic algorithms (GAs) and simulated annealing (SA). [1]

- **Genetic algorithms**: 

Genetic algorithms were invented by John Holland in the 1960s. Holland’s original goal was not to design application specific algorithms, but rather to formally study the ways of evolution and adaptation in nature and develop ways to import them into computer science. Holland [1975] presents the genetic algorithm as an abstraction of biological evolution and gives the theoretical framework for adaptation under the genetic algorithm. 

In order to explain genetic algorithms, some biological terminology needs to be clarified. All living organisms consist of cells, and every cell contains a set of chromosomes, which are strings of DNA and give the basic information of the particular organism. A chromosome can be further divided into genes, which in turn are functional blocks of DNA, each gene representing some particular property of the organism. The different possibilities for each property, e.g. different colors of the eye, are called alleles. Each gene is located at a particular locus of the chromosome. When reproducing, crossover occurs: genes are exchanged between the pair of parent chromosomes. The offspring is subject to mutation, where single bits of DNA are changed. The fitness of an organism is the probability that the organism will live to reproduce and carry on to the next generation. The set of chromosomes at hand at a given time is called a population. Genetic algorithms are a way of using the ideas of evolution in computer science. When thinking of the evolution and development of species in nature, in order for the species to survive, it needs to develop to meet the demands of its surroundings. Such evolution is achieved with mutations and crossovers between different chromosomes, i.e., individuals, while the fittest survive and are able to participate in creating the next generation.

In computer science, genetic algorithms are used to find a good solution from a very large search space, the goal obviously being that the found solution is as good as possible. To operate with a genetic algorithm, one needs an encoding of the solution, i.e., a representation of the solution in a form that can be interpreted as a chromosome, an initial population, mutation and crossover operators, a fitness function and a selection operator for choosing the survivors for the next generation.

- **Simulated annealing**:

Simulated annealing is originally a concept in physics. It is used when the cooling of metal needs to be stopped at given points where the metal needs to be warmed a bit before it can resume the cooling process. The same idea can be used to construct a search algorithm. At a certain point of the search, when the fitness of the solution in question is approaching a set value, the algorithm will briefly stop the optimizing process and revert to choosing a solution that is not the best in the current solution’s neighborhood. This way getting stuck to a local optimum can effectively be avoided. Since the fitness function in simulated annealing algorithms should always be minimized, it is usually referred to as a cost function. 

Simulated annealing optimally begins with a point x in the search space that has been achieved through some heuristic method. If no heuristic can be used, the starting point will be chosen randomly. The cost value c, given by cost function E, of point x is then calculated. Next a neighboring value x1 is searched and its cost value c1 calculated. If c1 < c, then the search moves onto x1. However, even though c <= c1, there is still a small chance, given by probability p that the search is allowed to continue to a solution with a bigger cost. The probability p is a function of the change in cost function delta of E, and a parameter T: p = e^(- delta of E/T). 

This definition for the probability of acceptance is based on the law of thermodynamics that controls the simulated annealing process in physics. The original function is p = e¨E/kt, where t is the temperature in the point of calculation and k is Boltzmann’s constant. The parameter T that substitutes the value of temperature and the physical constant is controlled by a cooling function C, and it is very high in the beginning of simulated annealing and is slowly reduced while the search progresses. The actual cooling function is application specific. If the probability p given by this function is above a set limit, then the solution is accepted even though the cost increases. The search continues by choosing neighbors and applying the probability function (which is always 1 if the cost decreases) until a cost value is achieved that is satisfactory low.

The parameter T that substitutes the value of temperature and the physical constant is controlled by a cooling function C, and it is very high in the beginning of simulated annealing and is slowly reduced while the search progresses. The actual cooling function is application specific.

- **NSAG-2**: 

(Non-dominated Sorting Genetic Algorithm II) [6] [7]. 
It is the current de facto standard for optimization of SE. This is a multi-objective evolutionary algorithm that uses non dominated sorting. An initial prototype was released in 1994-1995 which eliminates problems of scalarizing multiobjective problems by weights. Users were required to use domain knowledge to build the weights for objectives. It was found that the solutions were are highly sensitive to these magic weight vectors. 

- **GALE**: 

(Geometric active learning for Search-Based Software Engineering) [8]
GALE is a near-linear time MOEA that builds a piecewise approximation to the surface of best solutions along the Pareto frontier. For each piece, GALE mutates solutions towards the better end. In numerous case studies, GALE finds comparable solutions to standard methods (NSGA-II, SPEA2) using far fewer evaluations (e.g. 20 evaluations, not 1,000). GALE is recommended when a model is expensive to evaluate, or when some audience needs to browse and understand how an MOEA has made its conclusions.




#### Survey of related work: R

#### Trends: A

#### Applications:
- **Requirements Engineering**: 

Papers [4] and [5] give insight into research on application of SBSE in Requirements Selection and Optimisation. Most software product developments are iterative and incremental processes that are seldom completed in a single release. It is critical but challenging to select the requirements from a large number of candidates for the achievement of overall business goal and the goals of multiple stakeholders, each of whom may have competing and often conflicting priorities. This thesis argues that search-based techniques can be applied to the optimisation problem during the requirements selection and analysis phase for release planning problem. Search-based techniques offer significant advantages; they can be used to seek robust, scalable solutions, to investigate trade-offs, to yield insight and to provide feedback explaining choices to the decision maker. 

In the release planning for software development, the requirements interdependency relationship is an important element which reflects how requirements interact with each other in a software system. Furthermore, it also directly affects requirements selection activity as well as requirements traceability management, reuse and the evolution process. 

Requirements Engineering (RE) is an important branch of Systems Engineering (SE) which addresses a broad range of problems. It includes the identification of the objectives to be fulfilled, the acquisition and analysis of the requirements to be satisfied, the documentation of such requirements as the specifications, the validation and verification of the stakeholders’ needs to be met, as well as the management and support of the requirements processes and activities by using many notations, methodologies and techniques. One of the important tasks is requirements selection and optimisation. The goal is to identify optimal choices from all possible requirements and to explore tradeoff decision-making to satisfy the demands of stakeholders, while at the same time making sure that there are sufficient resources to undertake the selected task.

The intention of the [6] is to widen and explore the scope of the release planning as a problem within Search-based Software Engineering. The contributions were to provide new approaches to support and improve requirements analysis and optimisation. The motivation was to formulate requirements selection and analysis problems as search-based optimisation problems and, thereby, to provide an automatic, flexible framework. This may be useful for decision making support in the RE process. Based on the search-based requirements selection and optimisation framework, the contributions of the research work presented in the thesis can be summarised as follows:

1. **Value/Cost Trade-off in Requirements Selection**: Search techniques were applied to find approximations of the Pareto-optimal set. This allows the decision maker to select the preferred solution from the Pareto front according to different contexts. Empirical studies investigated the relative performance of the algorithms adopted in three scales of synthetic data sets and explored the ‘critical point’ at which the metaheuristic search techniques can outperform a random search. 
2. **Requirements Interaction Management**: Two empirical studies (a dependency impact study and a scale study) were conducted to simulate the requirements selection process under five common types of requirements dependencies (And, Or, Precedence, Value-related and Cost-related). The objectives were to investigate the influence of requirements dependence on the automated requirements selection process and to validate the feasibility of the proposed framework. In the empirical studies, the four data sets were designed in different scales and densities of the stakeholder-requirement matrix. The Dependency Impact Study evaluated the impacts of five different dependency types and the Scale Study reported results concerning the performance of the two algorithms (the NSGA-II algorithm and an archive based NSGA-II algorithm) as the data sets increase in size.
3. **Multi-Stakeholder Tensioning Analysis**: The study catered for tensioning of the relationship between the stakeholders and the resources as well as the natural internal tensioning among the stakeholders. We carried out an empirical study and a statistical analysis in which metaheuristic search techniques were applied to the new formulations of the problem. The results suggested that the Two-Archive algorithm outperforms the NSGA-II algorithm when the objectives increase in size. The study also provided Kiviat diagrams to visualise the discovered solutions in the multi-dimensional solution space. These scenarios let the decision maker explore and optimise the trade-offs allowed by the instantiation of the problem with which they are faced. We adapted the traditional static Kiviat diagram to create an animated Kiviat diagram that allows the decision maker to explore the dynamic changes in inter-stakeholder tension as budgetary pressure increases.

- **Debugging and Maintenance**: Paper [9] describes GenProg, an automated method for repairing defects in off-the-shelf, legacy programs without formal specifications, program annotations, or special coding practices. Software engineering is expensive, summing to over one half of one percent of the US GDP annually. Software maintenance accounts for over two-thirds of that life cycle cost, and a key aspect of maintenance is fixing bugs in existing programs. Unfortunately, the number of reported bugs far outstrips available development resources. It is common for a popular project to have hundreds of new bug reports filed every day. Many bugs can be fixed with just a few changes to a program's source code. Human repairs often involve inserting new code and deleting or moving existing code. GenProg uses those same building blocks to search for repairs automatically. Software maintenance is expensive. GenProg reduces software maintenance costs by automatically producing patches (repairs) for program defects. GenProg uses genetic programming to search for repairs. It's evolutionary computation represents candidate repairs as sequences of edits to software source code. Each candidate in a large population is applied to the original program to produce a new program, which is evaluated using test suites. Those candidates that pass more tests are said to have a higher fitness and are iteratively subjected to computational analogs of the biological processes of mutation and crossover. This process terminates when a candidate repair is found that retains all required functionality and fixes the bug. GenProg does not require special code annotations or formal specifications, and applies to unmodified legacy software.

GenProg has repaired many kinds of defects, including infinite loops, segmentation violations, heap buffer overflows, non-overflow denial-of-service vulnerabilities, stack buffer overflows, format string vulnerabilities, integer overflows — as well as the ubiquitous "my program produces the incorrect output" bug. While we evaluate primarily on C programs, GenProg can also repair programs in other languages, such as x86 or ARM assembly files or ELF binaries.

GenProg repaired 55 out of 105 bugs for $8 each in a recent systematic study that included multiple programs, totaling over 5 million lines of code, and supported by over 10,000 test cases.

- **Testing**: 

Paper [10] introduces three related algorithms and a tool, SWAT, for automated web application testing using Search Based Software Testing (SBST). The algorithms significantly enhance the efficiency and effectiveness of traditional search based techniques exploiting both static and dynamic analysis. Manual testing of the Web application is tedious but very important task. Search based Software engineering can help to generate test cases automatically. Although search based software engineering is used for test generation of traditional standalone application, web applications presents us with its own challenges. There are 4 main challenges faced for automatic test generation in web applications 
1. No consistent interface definition in the web page: There is no specification regarding how many i/p the program accepts or what are their type.
2. Dynamic typing: No information is present regarding data type of inputs
3. Identification of top level pages of application: for user simulation of the applicattion, we have to know the start page of the web application
4. Dynamic includes of other pages: The include name can be decided on runtime.

The author chose six PHP applications to test the new method of testing. The reason for choosing these projects is that they have already been used other research on Web testing using non search based approaches. The author provides reference [1] for this other research. 

The author provides significant results of the Branch coverage By 3 different Algorithms, each of which is run 30 times. These results can be directly used in future studies to compare the performance of other enhancements or alterations of the used Algorithms in the study.



- Optimizing software: R
- Project Management: A

#### Limitations of SBSE: A

#### Future Work and improvements: R

#### References: 
1. A Survey on Search-Based Software Design, O Räihä - Computer Science Review, 2010 - Elsevier
2. Novel sorption-based methodologies for static microextraction analysis: a review on SBSE and related techniques JMF Nogueira - Analytica Chimica Acta, 2012 - Elsevier
3. Search based software engineering: A comprehensive analysis and review of trends techniques and applications, M Harman, SA Mansouri, Y Zhang - … London, Tech. Rep. TR-09-03, 2009 
4. Multi-Objective Search-based Requirements Selection and Optimisation, Zhang, Yuanyuan (February 2010)
5. Search Based Optimization of Requirements Interaction Management, Yuanyuan Zhang, Mark Harman, Soo Ling Lim (April 2011)
6. A fast and elitist multiobjective genetic algorithm: NSGA-II, Deb, K. ; Kanpur Genetic Algorithms Lab., Indian Inst. of Technol., Kanpur, India ; Pratap, A. ; Agarwal, S. ; Meyarivan, T.
7. Dr. Menzies's open source notes on SBSE: https://github.com/timm/sbse14/wiki/nsgaii
8. Dr. Menzies's notes ont GALE, http://www.slideshare.net/timmenzies/gale-geometric-active-learning-for-searchbased-software-engineering
9. GenProg: A generic method for automatic software repair, Le Goues, C. ; Dept. of Comput. Sci., Univ. of Virginia, Charlottesville, VA, USA ; ThanhVu Nguyen ; Forrest, S. ; Weimer, W.
10. Automated Web Application Testing Using Search Based Software Engineering. In ASE '11 Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering. By Nadia Alshahwan and Mark Harman, 2011. 
11. 
